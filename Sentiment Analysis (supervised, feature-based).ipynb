{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (supervised, feature-based)\n",
    "\n",
    "This tutorial shows sentiment analysis using feature extraction and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# The next imports are only needed for the preprocessing stemming function from nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from utils.nlputils import preprocess_text\n",
    "from text_preprocessing import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load traing data\n",
    "\n",
    "Use `pandas` to read files. Here i reference a file that conains 703 tweets together with a polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@united UA5396 can wait for me. I'm on the gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate Time Warner! Soooo wish I had Vios. Can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom Shanahan's latest column on SDSU and its N...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Found the self driving car!! /IWo3QSvdu2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@united arrived in YYZ to take our flight to T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  senti\n",
       "0  @united UA5396 can wait for me. I'm on the gro...      0\n",
       "1  I hate Time Warner! Soooo wish I had Vios. Can...      0\n",
       "2  Tom Shanahan's latest column on SDSU and its N...      2\n",
       "3           Found the self driving car!! /IWo3QSvdu2      2\n",
       "4  @united arrived in YYZ to take our flight to T...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_train = pd.read_csv('data/twitter-sentiment/twitter-sentiment-bowden-training.csv')\n",
    "\n",
    "# Print the first 5 lines\n",
    "df_tweets_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for \"senti\", i.e., the labels are:\n",
    "- 0 means negative\n",
    "- 2 means neutral\n",
    "- 4 means positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `pandas` data frame,  generate two list, one containing the tweets, the other containg the polarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities_train = df_tweets_train['senti'].tolist() \n",
    "tweets_train = df_tweets_train['tweet'].tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess tweets\n",
    "\n",
    "Use `preprocess_text()` method to preprocess all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess_text() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f1ec0581cd87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprocessed_tweets_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtweet_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwordnet_lemmatizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess_text() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "processed_tweets_train = [''] * len(tweets_train)\n",
    "\n",
    "for idx, doc in enumerate(tweets_train):\n",
    "    processed_tweets_train[idx] = preprocess_text(doc, tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate feature set\n",
    "\n",
    " Use `TfidfVectorizer` to calculate the document word matrix which will be the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-eccf6bea94af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_tweets_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(processed_tweets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of classifier\n",
    "\n",
    "Packages like `scikit-learn` make it extremely easy to train machine learning classifiers. `scikit-learn` offers a variety of classifier algorithms. Here, i use `MultinomialNB`, a Multinomial Naive Bayes classifier which is often a good choice in case of text documents. \n",
    "\n",
    "But feel free to try other classifiers! \n",
    "\n",
    "Very conveniently, all `scikit-learn` classifiers use the same methods which makes replacinf classifiers very quick and easy. For the full list of supported classifiers, see the `sklearn-learn` website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5eb80d0d830a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolarities_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#clf = DecisionTreeClassifier().fit(X_train_tfidf, polarities_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#clf = LinearSVC().fit(X_train_tfidf, polarities_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#clf = KNeighborsClassifier().fit(X_train_tfidf, polarities_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, polarities_train)\n",
    "#clf = DecisionTreeClassifier().fit(X_train_tfidf, polarities_train)\n",
    "#clf = LinearSVC().fit(X_train_tfidf, polarities_train)\n",
    "#clf = KNeighborsClassifier().fit(X_train_tfidf, polarities_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the classifier with 2 sample documents to see the 2 basic required steps:\n",
    "\n",
    "- convert documents into the document word matrix; note that we have to make sure that the matrix (with respect to the vocabulary) with the one generated from the training data\n",
    "\n",
    "- run classifier over the document word matrix (i.e., the feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['soccer is so much fun', 'being hungry is shit']\n",
    "\n",
    "# Use the fitted vectorizer to transform the documents: transform() not fit_transform()!\n",
    "X_new_tfidf = tfidf_vectorizer.transform(docs_new)\n",
    "\n",
    "# Use the trained classifier to predict the polarities\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('{} => {}'.format(doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data\n",
    "\n",
    "Now perform the exact same steps as i did for the training data:\n",
    "\n",
    "- load the file with the test data\n",
    "\n",
    "- convert `pandas` data frame into two lists (tweets + polarities)\n",
    "\n",
    "- preprocess tweets (the same way we did the tweets of the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_test = pd.read_csv('data/twitter-sentiment/twitter-sentiment-bowden-test.csv')\n",
    "\n",
    "polarities_test = df_tweets_test['senti'].tolist() \n",
    "tweets_test = df_tweets_test['tweet'].tolist() \n",
    "\n",
    "processed_tweets_test = [''] * len(tweets_test)\n",
    "\n",
    "for idx, doc in enumerate(tweets_test):\n",
    "    processed_tweets_test[idx] = preprocess_text(doc, tokenizer=tweet_tokenizer, lemmatizer=wordnet_lemmatizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform test\n",
    "\n",
    "The last two steps are as outline above:\n",
    "\n",
    "- convert documents into the document word matrix; note that we have to make sure that the matrix (with respect to the vocabulary) with the one generated from the training data\n",
    "\n",
    "- run classifier over the document word matrix (i.e., the feature set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fitted vectorizer to transform the documents: transform() not fit_transform()!\n",
    "X_test_tfidf = tfidf_vectorizer.transform(tweets_test)\n",
    "\n",
    "predicted = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "`scikit-learn` provides a series of methods calculate a variety of metrics, particularly the precision, recall, and f1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(polarities_test, predicted))\n",
    "print()\n",
    "print(classification_report(polarities_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use additional features (optional)\n",
    "\n",
    "This example shall illustrate that a feature vector can contain all kinds of dimensions/features. So far we only used the feature vectorss derived from the words. In the following, add to this feature vector two more values which we get from from the unsupervised sentiment analysis to boost the classifier.\n",
    "\n",
    "Remember, for the unsupervised sentiment analysis, we assinged it word with a positive or negative numerical value, and summed up all values within document to get the final sentiment values. We can use this information to add to the feature vector. Since the values must be positive, we add 2 dimensions to the vector: one indicating that the sentiment score was positive and one indicating that score was negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised sentiment analysis step\n",
    "\n",
    "We copy the folling part from the tutorial for the unsupervised sentiment analysis:\n",
    "\n",
    "- load sentiment lexicon\n",
    "\n",
    "- define method `calc_polarity` that calculates the polarity of a document depending on the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('data/sentiment-lexicon/sentilex-vader.txt', sep='\\t', encoding = \"ISO-8859-1\", header=None)\n",
    "\n",
    "sentiment_dict = {}\n",
    "\n",
    "for index, row in df_sentiment.iterrows():\n",
    "    token, score = row[0], row[1]\n",
    "    sentiment_dict[token] = score / 4.0 # normalize score from [-4,...,4] to [-1,...,1]\n",
    "\n",
    "def calc_polarity(doc, num_polarities=3):\n",
    "    doc_score = 0.0\n",
    "    for token in doc.split(): # Here split() is sufficient\n",
    "        if token in sentiment_dict:\n",
    "            doc_score += sentiment_dict[token]\n",
    "    if doc_score > 0:\n",
    "        return 1.0\n",
    "    elif doc_score < 0:\n",
    "        return -1.0\n",
    "    else:\n",
    "        if num_polarities == 3:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1.0\n",
    "    return 0.0 # Just to be sure, should never be reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "The next lines cover these 3 main steps:\n",
    "\n",
    "- Calculate the basic feature set using the `TfidfVectorizer`, same as above\n",
    "\n",
    "- Calculate the extended feature set with 2 dimension for each feature vector\n",
    "\n",
    "- Merge both feature sets into one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(processed_tweets_train)\n",
    "\n",
    "print (type(X_train_tfidf))\n",
    "print (X_train_tfidf.shape)\n",
    "\n",
    "new_feature_col_train = csr_matrix((X_train_tfidf.shape[0], 2), dtype=float)\n",
    "\n",
    "print (new_feature_col_train.shape)\n",
    "\n",
    "for idx, doc in enumerate(tweets_train):\n",
    "    polarity = calc_polarity(doc)\n",
    "    if polarity > 0:\n",
    "        new_feature_col_train[idx,0] = 1\n",
    "    elif polarity < 0:\n",
    "        new_feature_col_train[idx,1] = 1  \n",
    "    #new_feature_col_train[idx] = ((calc_polarity(doc) + 1.0) / 2.0)\n",
    "\n",
    "# Merge the 699x2677 matrix with the 699x2 matrix\n",
    "X_train_tfidf = hstack((X_train_tfidf, new_feature_col_train))\n",
    "\n",
    "print (X_train_tfidf.shape)\n",
    "print (len(polarities_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating classifier\n",
    "\n",
    "From here on, everything is the same as above, only that the features set of the test data needs also be extended, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, polarities_train)\n",
    "#clf = DecisionTreeClassifier().fit(X_train_tfidf, polarities_train)\n",
    "#clf = LinearSVC().fit(X_train_tfidf, polarities_train)\n",
    "#clf = KNeighborsClassifier().fit(X_train_tfidf, polarities_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(tweets_test)\n",
    "\n",
    "new_feature_col_test = csr_matrix((X_test_tfidf.shape[0], 2), dtype=float)\n",
    "\n",
    "for idx, doc in enumerate(tweets_test):\n",
    "    polarity = calc_polarity(doc)\n",
    "    if polarity > 0:\n",
    "        new_feature_col_test[idx,0] = 1\n",
    "    elif polarity < 0:\n",
    "        new_feature_col_test[idx,1] = 1  \n",
    "    #new_feature_col_train[idx] = ((calc_polarity(doc) + 1.0) / 2.0)\n",
    "\n",
    "print (new_feature_col_test.shape)\n",
    "\n",
    "X_test_tfidf = hstack((X_test_tfidf, new_feature_col_test))\n",
    "\n",
    "predicted = clf.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(polarities_test, predicted))\n",
    "print()\n",
    "print(classification_report(polarities_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
